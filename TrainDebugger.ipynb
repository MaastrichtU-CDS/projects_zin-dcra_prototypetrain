{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Railway Train Debugger\n",
    "Read the README.MD and FLOW.MD for more detailed explainations"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "from CentralMock import *\n",
    "centralMock = CentralMock()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single run simulations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completedTaskDtos = []\n",
    "completedTaskDtos.append(TaskDto())\n",
    "completedTaskDtos[0].id = 1\n",
    "completedTaskDtos[0].calculationStatus = 'COMPLETED'\n",
    "completedTaskDtos[0].stationId = 2\n",
    "completedTaskDtos[0].result = json.dumps({'calculation_result': 4})\n",
    "completedTaskDtos[0].iteration = 0\n",
    "completedTaskDtos.append(TaskDto())\n",
    "completedTaskDtos[1].id = 2\n",
    "completedTaskDtos[1].calculationStatus = 'COMPLETED'\n",
    "completedTaskDtos[1].stationId = 3\n",
    "completedTaskDtos[1].result =  json.dumps({'calculation_result': 6})\n",
    "completedTaskDtos[1].iteration = 0\n",
    "\n",
    "centralMock.clearTmpFolder()\n",
    "centralMock.clearTrainFolder()\n",
    "\n",
    " # For the demo the input.txt is JSON format. But you are free to use any string format, as long as your applications knows what the format is.\n",
    "print(\"run master final iteration\")\n",
    "inputStr = json.dumps({\"iterations\": 0})\n",
    "outputStr, newTasks = centralMock.perfomMasterTask(inputStr, completedTaskDtos)\n",
    "print(outputStr)\n",
    "centralMock.archiveRun(True, 0, 0)\n",
    "centralMock.clearTmpFolder()\n",
    "\n",
    "print(\"run master some iteration\")\n",
    "inputStr = json.dumps({\"iterations\": 1})\n",
    "outputStr, newTasks  = centralMock.perfomMasterTask(inputStr, completedTaskDtos)\n",
    "print(newTasks)\n",
    "centralMock.archiveRun(True, 99, 99)\n",
    "centralMock.clearTmpFolder()\n",
    "\n",
    "print(\"run station iteration\")\n",
    "inputStr = json.dumps({'number_to_process': 3})\n",
    "outputStr = centralMock.perfomStationTask(inputStr)\n",
    "print(outputStr)\n",
    "centralMock.archiveRun(False, 0, 0)\n",
    "centralMock.clearTmpFolder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform computations for train\n",
    "- Create a list of completed tasks for the stations you want to use \n",
    "- If you need more stations extend this list\n",
    "- This module mocks the Central behaviour that triggers a train\n",
    "- To demonstrate how the input works the number of iterations for this demo is set as an input variable for the prototype train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Starting Task:\n{\"result\": \"{\\\"calculation_result\\\": 1}\", \"input\": \"\", \"calculationStatus\": \"COMPLETED\", \"stationId\": 2, \"iteration\": 0}\nStarting Task:\n{\"result\": \"{\\\"calculation_result\\\": 2}\", \"input\": \"\", \"calculationStatus\": \"COMPLETED\", \"stationId\": 3, \"iteration\": 0}\nstarting master task for iteration: 0\nnew run detected, set the variables for the new run\nStarting Task:\n{\"result\": \"\", \"input\": \"{\\\"number_to_process\\\": 1}\", \"calculationStatus\": \"REQUESTED\", \"stationId\": 2, \"iteration\": 1}\nStarting Task:\n{\"result\": \"\", \"input\": \"{\\\"number_to_process\\\": 2}\", \"calculationStatus\": \"REQUESTED\", \"stationId\": 3, \"iteration\": 1}\nstarting master task for iteration: 1\nfinal master run detected\n{\"total_sum\": 6.0}\nsimulation is done\n"
    }
   ],
   "source": [
    "taskDtos = []\n",
    "taskDtos.append(TaskDto())\n",
    "taskDtos[0].id = 1\n",
    "taskDtos[0].calculationStatus = 'COMPLETED'\n",
    "taskDtos[0].stationId = 2\n",
    "taskDtos[0].result = json.dumps({'calculation_result': 1})\n",
    "taskDtos[0].iteration = 0\n",
    "taskDtos.append(TaskDto())\n",
    "taskDtos[1].id = 2\n",
    "taskDtos[1].calculationStatus = 'COMPLETED'\n",
    "taskDtos[1].stationId = 3\n",
    "taskDtos[1].result =  json.dumps({'calculation_result': 2})\n",
    "taskDtos[1].iteration = 0\n",
    "\n",
    "centralMock.clearTmpFolder()\n",
    "centralMock.clearTrainFolder()\n",
    "\n",
    "ITERATIONS = 2\n",
    "MASTER_STATION_ID = 1\n",
    "TRAIN_DONE = False\n",
    "\n",
    " # For the demo the input.txt is JSON format. But you are free to use any string format, as long as your applications knows what the format is.\n",
    "masterInputStr = json.dumps({\"iterations\": ITERATIONS})\n",
    "# centralMock.clearTmpFolder()\n",
    "\n",
    "iteration = 0\n",
    "iteration_safety = 9 # break after 9 iterations, prevent infinite while\n",
    "while not TRAIN_DONE:\n",
    "    for taskDto in taskDtos:\n",
    "        # loop over the tasks that should be done\n",
    "        print('Starting Task:')\n",
    "        print(json.dumps(taskDto.export()))\n",
    "        if taskDto.calculationStatus != 'COMPLETED':\n",
    "            taskDto.result = centralMock.perfomStationTask(taskDto.inputStr)\n",
    "            taskDto.calculationStatus = 'COMPLETED'\n",
    "            centralMock.archiveRun(False, taskDto.iteration, taskDto.stationId)\n",
    "            #TODO add error handling, if error log is filled \n",
    "\n",
    "    print('starting master task for iteration: ' + str(taskDtos[0].iteration))\n",
    "    masterOutputStr, newTaskDtos = centralMock.perfomMasterTask(masterInputStr, taskDtos)\n",
    "    centralMock.archiveRun(True, taskDtos[0].iteration, MASTER_STATION_ID)\n",
    "\n",
    "    if newTaskDtos.__len__() == 0:\n",
    "        print('final master run detected')\n",
    "        print(masterOutputStr)\n",
    "        TRAIN_DONE = True\n",
    "    else:\n",
    "        print('new run detected, set the variables for the new run')\n",
    "        taskDtos = []\n",
    "        for item in newTaskDtos:\n",
    "            taskDtos.append(TaskDto(item))\n",
    "        masterInputStr = masterOutputStr\n",
    "    \n",
    "    iteration = iteration + 1\n",
    "    if iteration > iteration_safety:\n",
    "        print('error break out of while')\n",
    "        TRAIN_DONE = True\n",
    "\n",
    "        \n",
    "print('simulation is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# print(datetime.datetime.now())\n",
    "# stream = subprocess.call('docker exec test_train sleep 5')\n",
    "\n",
    "# # stream.wait()\n",
    "# print(datetime.datetime.now())\n",
    "\n",
    "# output = stream.stdout\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = centralMock.exportTaskDtoWithId(completedTasks[1])\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED_TASK_PATH = './tmp/completed-client-tasks.json'\n",
    "# subprocess.call('docker cp ' + COMPLETED_TASK_PATH + ' test_train:/opt/completed-client-tasks.json', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.lexists('./tmp/abc12/abc2/a'):\n",
    "    os.makedirs('./tmp/abc12/abc2/a')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completedTaskDtos[0].inputStr)\n",
    "\n",
    "for dto in completedTaskDtos:\n",
    "    dto.inputStr = 'test'\n",
    "\n",
    "print(completedTaskDtos[0].inputStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}